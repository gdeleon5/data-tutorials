---
title: Image Generation and Recognition with TF
author: sandbox-team
tutorial-id: xxx
experience: Beginner
persona: Data Scientist
source: Hortonworks
use case: Example (Data Discovery)
technology: TensorFlow
release: hdp-3.0.1
environment: Sandbox
product: HDP
series: TBD
---

# Generating and Labeling Images with TF

## Introduction

In this tutorial we will explore at a very hight level the role that  Generative Adversarial Networks or GANs play in the generation of images while leveraging Hortonworks Data Platform(HDP).

## Prerequisites

- Downloaded and deployed the [Hortonworks Data Platform (HDP)](https://hortonworks.com/downloads/#sandbox) Sandbox
- [TensorFlow on Yarn](http://hortonworks.com/tutorial/tensorflow-on-yarn)
- [Object Detection in 5 Minutes](http://hortonworks.com//tutorial/object-detection-in-5-minutes)
- [Model Retraining for Object Recognition](http://hortonworks.com//tutorial/model-retraining-for-object-recognition)

## Outline

- [Concepts](#concepts)
- [Environment Setup](#environment-setup)
- [Implement the Image Generation Model](#implement-the-image-generation-model)
- [Summary](#summary)
- [Further Reading](#further-reading)
- [Appendix A: Upload and Image and Compare the Loss Values](#appendix-a-upload-an-iamge-and-compare-the-loss-values)

## Concepts

### GANs

Amongst the plethora of Machine learning neural networks there has been one network that has not ceased to generate fuzz. This very famous network is the Generative Neural Network or GANs. GANs was created in 2014 by Goodfellow, refer to the **Further Reading** section to learn more about GANs from the source.

At a very high level, the GAN architecture is composed of two connected neural networks, a generator and a discriminator that are engaged in an epic zero-sum battle. Both networks start with an original set of data, in this case we will use images.

On one side there is the generator network who's sole purpose is to trick the discriminator network. The generator trains with the original data set and generate images from noise that can fool the discriminator to think that the images are from the original data set. Meanwhile the discriminator trains with the original data set to detect any fakes that generator sends its way and gives the probability that the image given is real or fake.

Both Networks are initially trained separately then put up against each other, during this process each network learns to either create better fakes or discriminate the real from the fake. From a perspective of statistics, was the image set generated X = {x<sub>1</sub>,...,x<sub>n</sub>} and X<sup>'</sup>={x<sup>'</sup><sub>1</sub>,...,x<sup>'</sup><sub>n</sub>} drawn from the same distribution? If not,the Generative Network will train to create data that resembles the distribution.

This is a sample of the GAN architecture:
![gan-architecture](assets/gan-architecture.jpg )
*Image from [Towards Science](https://towardsdatascience.com/generative-adversarial-networks-gans-a-beginners-guide-5b38eceece24)*

The two image below are examples of the images that were generated by [Goodfellow's Tutorial on GANs](https://arxiv.org/pdf/1701.00160.pdf).

The GANs created the faces by using the Toronto Face Dataset:
![gan-face-generation](assets/gan-face-generation.jpg)

 Another GAN example:

![gan-dog-image](assets/gan-dog-image.jpg)

Learn more about GANs in the [Further Reading](#further-reading) section of this tutorial.

### Environment Setup

The following libraries need to be installed:

- python3
- numpy
- matplotlib
- tf-hub
- tensorflow

Open your terminal and check for the following dependencies:
```
pip3 --version
python3 --version   //minimum3.6
virtualenv --version
```

If you have not done so install the libraries below:

```
sudo apt install python3-dev python3-pip
sudo pip3 install -U virtualenv
```

The virtual environment will be set up from the home directory:
```
virtualenv --system-site-packages -p python3 ./venv 
source ./venv/bin/activate    #activate the virtual environment
```

At this point cd into the **venv** directory and export the libraries below.
```
cd ~/venv
/usr/bin/ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"
export PATH="/usr/local/bin:/usr/local/sbin:$PATH"
```

```
pip3 install numpy
pip3 install matplotlib
pip3 install tensorflow
pip3 install tensorflow-hub
```

```
pip3 install --upgrade tensorflow
python3 -m pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.12.0-py3-none-any.whl
```

## Implement the Image Generation Model

We will use a pre-trained GANs model from TensorFlow Hub that was trained using TF-Hub module [program-128](https://tfhub.dev/google/progan-128/1), **CelebA** dataset, on a GPU of 636,801 steps with a batch size of 16.

>Note: The python code for the GAN and CNN models are slight modifications of Google's Colab [Generating Images](https://github.com/tensorflow/hub/blob/master/examples/colab/tf_hub_generative_image_module.ipynb)

Running this pre-trained model will generate two outputs: a GIF which is an example of latent space interpolation between two vectors and gradient descent variable values needed to reduce loss.

### Install Dependencies for the Image Generator

TF-Hub Generative Image Model

In your terminal go to the **ven** directory you created.

```
cd ~/venv
```

Install these additional libraries:

```
pip3 install imageio
pip3 install scikit-image
pip3 install matplotlib.pyplot as plt
pip3 install ipython
pip3 install numpy
/Applications/Python\ 3.6/Install\ Certificates.command
```

Download the python code to generate celebrity images.

>Note: Learn more about Google's Colab [Image Generation Python Script](https://github.com/tensorflow/hub/blob/master/examples/colab/tf_hub_generative_image_module.ipynb)

~~~
wget github.com/ImageGenerator.py
~~~

### Run the image generator code:

If you have not done so, activate the virtual environment **venv** that was just created.

~~~
source ~venv/bin/activate
~~~

Execute the python script:
~~~
python3 ImageGenerator.py
~~~

### Results

The following results will appear shortly after you run the script: a GIF and gradient descent loss values.

The GIF below was generated as a result of the pre-trained GAN model and TF-Hub. It highlights the latent space interpolation between random vectors. We can think of the latent space as the compressed representation of the original image(s), from these values the new images are generated with the goal of generating images close as possible as the originals. Learn more about [latent variables and GANs](https://jhui.github.io/2017/03/05/Generative-adversarial-models/).

![animation](assets/animation.gif)

On your terminal you will also see the following values. These values were obtained by first finding the loss function between a target image in this case and image from the **CelebrityAs** dataset and the  generated image. Gradient descent was then used in the loss function which generated the values that would minimize the loss. Gradient descent is a optimization algorithm used to find the minimum of a function or the point where the loss function converges. Learn more about [gradient descent](https://developers.google.com/machine-learning/crash-course/reducing-loss/gradient-descent).

In this case because an image from the same dataset was used the descent to the minimum was quick. These can be noted from the values below.

~~~
~/venv
$python3 ImageGenerator.py
2018-12-19 17:55:19.249869: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
<IPython.core.display.Image object>
0.68876916
5.607328
5.049135
2.7256749
0.7315421
1.4969518
0.8285036
1.1469646
1.4737974
0.78105783
0.87805945
1.2609681
0.8296515
0.5128517
0.7658002
0.2812683
1.0127488
1.3108158
0.95765066
0.1757649
0.3891008
0.22717363
0.20078099
0.379921
0.14679064
0.8292588
0.9534563
0.5001564
0.641171
0.94268334
0.6888621
0.25469738
0.40378252
0.17268972
0.18588635
0.356967
0.16601633
0.6979192
0.79671913
0.37014747
(venv)
~~~

Let's try an image of an animated celebrity and see what values we get.We will use the  an animated version of a legend in the culinary world, Gordon Ramsay. We will use Mr. Ramsay's Simpson-style animated image as the target image and compare the loss values. Full credit to the Simpson's animators for the Simpson-style Gordon Ramsay image below.

![gordon-ramsay](assets/gordon-ramsay.jpg)

Download the image into the **venv** directory where you have the ImageGenerator.py code.

~~~
wget github.com/gordon-ramsay.jpg
~~~

The python script needs to be slightly modified in order to use an external image outside of the dataset. Download the modified python script.

~~~
wget github.com/ImageGeneratorExternalImage.py
~~~

Run the code:

~~~
python3 ImageGeneratorExternalImage.py
~~~

You should see results similar to the ones below. Notice how it takes a slightly longer for the loss function to converge.

~~~
~/venv
$python3 ImageGenerator.py
2018-12-29 18:22:05.498454: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
<IPython.core.display.Image object>
0.98289144
5.789063
5.1794066
2.8688426
0.59100956
1.3075445
0.60824585
1.4375224
1.7986321
1.1370378
0.53862023
0.8939043
0.4314929
0.966286
1.2422816
0.77658486
0.525299
0.80663085
0.4287827
0.75106114
0.9834588
0.5788738
0.58368105
0.8411632
0.51138973
0.5612605
0.7653878
0.3916356
0.6961323
0.9382956
0.6357596
0.37275413
0.56090117
0.20964515
0.8261446
1.0553855
0.768913
0.19808885
0.37734875
0.1718393
(venv)
~~~

## Summary

Congratulations, you have successfully generated images using a pre-trained GANs model. You generated a GIF of Celebrity images and tested how quickly the loss function converged while using an image from the original dataset and an image outside the dataset.

## Further Reading 

- [Generative Adversarial Nets by Ian Goodfellow](https://arxiv.org/pdf/1406.2661.pdf)
- [Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](https://arxiv.org/abs/1511.06434)
- [A Beginner's Guide to Generative Adversarial Networks(GANs)](https://skymind.ai/wiki/generative-adversarial-network-gan)
- [Generative Adversarial Networks- History and Overview](https://towardsdatascience.com/generative-adversarial-networks-history-and-overview-7effbb713545)
- [GAN Lab](https://poloclub.github.io/ganlab/)
- [The Mostly Complete Chart of Neural Networks, explained](https://towardsdatascience.com/the-mostly-complete-chart-of-neural-networks-explained-3fb6f2367464)
- [TensorFlow](https://www.tensorflow.org/)
- [TensorFlow Hub](https://www.tensorflow.org/hub/)

## Appendix A: Upload and Image and Compare the Loss Values

Try your own image. Upload an image and compare your results with those obtained when using a target image from the original dataset.

Simply upload the image to the venv directory and run the ImageGenerator.py script.

~~~
python3 ImageGeneratorExternalImage.py --image "<name of image>"
~~~